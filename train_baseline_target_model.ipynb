{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449aee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import optuna\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from src.save_load import buildTargetMetadata, saveTarget, saveTargetSignals\n",
    "from src.train_models import trainTargetModel, trainFbDTargetModel\n",
    "from src.visualize_model import VisualizeModel\n",
    "from src.utils import print_yaml, calculate_logits_and_inmask, rescale_logits\n",
    "from src.models.resnet18_model import ResNet18\n",
    "from src.models.wideresnet28_model import WideResNet\n",
    "from src.cifar_handler import CifarInputHandler\n",
    "from src.dataset_handler import processDataset, loadDataset, get_dataloaders\n",
    "\n",
    "from LeakPro.leakpro.attacks.mia_attacks.rmia import rmia_get_gtlprobs\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- #\n",
    "#   Load training yaml   #\n",
    "# ---------------------- #\n",
    "config = None\n",
    "with open(\"./train.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "train_cfg = config[\"train\"]\n",
    "data_cfg = config[\"data\"]\n",
    "    \n",
    "print(f\"Initial training config: {train_cfg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ #\n",
    "#  Load study + best trial info  #\n",
    "# ------------------------------ #\n",
    "study_folder = \"cinic10-resnet-baseline-6d2602e58b\"\n",
    "study_path = os.path.join(\"study\", study_folder)\n",
    "\n",
    "journal_storage = False\n",
    "if journal_storage:\n",
    "    # Journal + storage\n",
    "    journal_path = os.path.join(study_path, \"journal.log\")\n",
    "    storage = JournalStorage(JournalFileBackend(journal_path))\n",
    "\n",
    "    # Load study\n",
    "    study_name = metadata[\"study\"][\"study_name\"]\n",
    "    study = optuna.load_study(storage=storage, study_name=study_name)\n",
    "else:\n",
    "    db_path = os.path.join(\"study\", \"baseline_study.db\")\n",
    "    storage = f\"sqlite:///{db_path}\"\n",
    "    study_name = study_folder\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    print(f\"Quantity of trials: {len(study.trials)}\")\n",
    "\n",
    "# Metadata\n",
    "metadata_path = os.path.join(study_path, \"metadata.json\")\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    study_metadata = json.load(f)\n",
    "\n",
    "# Best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "print(\"Best trial parameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ------------------------------ #\n",
    "#  Update training configuration #\n",
    "# ------------------------------ #\n",
    "\n",
    "# Overwrite study-optimized params\n",
    "train_cfg[\"batch_size\"] = best_trial.params[\"batch_size\"]\n",
    "train_cfg[\"learning_rate\"] = best_trial.params[\"lr\"]\n",
    "train_cfg[\"momentum\"] = best_trial.params[\"momentum\"]\n",
    "train_cfg[\"weight_decay\"] = best_trial.params[\"weight_decay\"]\n",
    "train_cfg[\"t_max\"] = best_trial.params.get(\"T_max\", None)  # fallback default\n",
    "train_cfg[\"model\"] = study_metadata[\"study\"][\"model\"]   # Model architecture from metadata\n",
    "train_cfg[\"drop_rate\"] = best_trial.params.get(\"drop_rate\", None)  # fallback default\n",
    "train_cfg[\"optimizer\"] = best_trial.params.get(\"optimizer\", \"SGD\")\n",
    "\n",
    "print(\"Modified train_cfg with optimal hyperparameters\")\n",
    "\n",
    "config[\"train\"] = train_cfg\n",
    "\n",
    "# ------------------- #\n",
    "#  Save the metadata  #\n",
    "# ------------------- #\n",
    "train_metadata = buildTargetMetadata(train_cfg, data_cfg)\n",
    "hash_id, save_dir = saveTarget(train_metadata)\n",
    "print(f\"Saved training metadata with {hash_id} at {save_dir}\")\n",
    "print(\"-------------------- New config --------------------\")\n",
    "print_yaml(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "#   Prepare dataset   #\n",
    "# ------------------- #\n",
    "print(\"-------------------- Data_cfg --------------------\")\n",
    "print_yaml(data_cfg)\n",
    "\n",
    "batch_size = train_cfg[\"batch_size\"]\n",
    "\n",
    "trainset, testset, full_dataset = loadDataset(data_cfg)\n",
    "train_dataset, test_dataset, train_indices, test_indices = processDataset(data_cfg, trainset, testset, dataset=full_dataset)\n",
    "# Perpare loaders\n",
    "train_loader, test_loader = get_dataloaders(batch_size, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ee265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- #\n",
    "#   Train Baseline Target model   #\n",
    "# ------------------------------- #\n",
    "train_result, test_result = trainTargetModel(config, train_loader, test_loader, train_indices, test_indices, save_dir)\n",
    "if(train_result != None and test_result != None):\n",
    "    VisualizeModel().visualize(train_result, test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#   Load saved model and dataset   #\n",
    "# -------------------------------- #\n",
    "\n",
    "# Load the target\n",
    "reload_target = True\n",
    "if reload_target:\n",
    "\n",
    "    target = \"resnet-cinic10-c51d329813\"\n",
    "    target_path = os.path.join(\"target\", target)\n",
    "\n",
    "    metadata_path = os.path.join(target_path, \"metadata.json\")\n",
    "    # Load metadata.json\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        \n",
    "    # Load and Recreate the model\n",
    "    target_model_pkl_path = os.path.join(target_path, \"target_model.pkl\")\n",
    "    \n",
    "    state_dict = torch.load(target_model_pkl_path, map_location=\"cpu\")\n",
    "    if metadata[\"data\"][\"dataset\"] == \"cifar10\" or metadata[\"data\"][\"dataset\"] == \"cinic10\":\n",
    "        num_classes = 10 # or 100 in case of cifar 100\n",
    "    else:\n",
    "        num_classes = 100\n",
    "        \n",
    "    if metadata[\"train\"][\"model\"] == \"resnet\":\n",
    "        model = ResNet18(num_classes=num_classes)\n",
    "    elif metadata[\"train\"][\"model\"] == \"wideresnet\":\n",
    "        model = WideResNet(depth=28, num_classes=num_classes, widen_factor=10)\n",
    "   \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_model_pkl_path = os.path.join(target_path, \"model_metadata.pkl\")       \n",
    "    with open(metadata_model_pkl_path, \"rb\") as f:\n",
    "        metadata_pkl = pickle.load(f)\n",
    "\n",
    "# ---------------------------- #\n",
    "#   Load Dataset from Pickle   #\n",
    "# ---------------------------- #\n",
    "reload_dataset = True\n",
    "if reload_dataset:\n",
    "\n",
    "    dataset_name = metadata[\"data\"][\"dataset\"] + \".pkl\"\n",
    "    data_path = \"data\"\n",
    "\n",
    "    dataset_pkl_path = os.path.join(data_path, dataset_name)       \n",
    "    with open(dataset_pkl_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    # Wrap the full dataset in UserDataset if needed\n",
    "    if not isinstance(dataset, CifarInputHandler.UserDataset):\n",
    "        data_tensor, target_tensor = dataset\n",
    "        dataset = CifarInputHandler.UserDataset(data_tensor, target_tensor)\n",
    "        \n",
    "    labels = dataset.targets\n",
    "    \n",
    "train_indices = metadata_pkl.train_indices\n",
    "test_indices = metadata_pkl.test_indices\n",
    "print(f\"First 10 labels: {labels[:10]}, length: {len(labels)}\")\n",
    "print(f\"Train_indices length: {len(train_indices)}, test_indices length: {len(test_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------- #\n",
    "#   Calculate and save Logits and in_mask   #\n",
    "# ----------------------------------------- #\n",
    "logits, in_mask = calculate_logits_and_inmask(dataset, model, metadata_pkl, target_path, idx=None, save=False)\n",
    "if labels is not None:\n",
    "    resc_logits = rescale_logits(logits, labels)\n",
    "    gtl_probs = rmia_get_gtlprobs(logits, labels)\n",
    "saveTargetSignals(logits, in_mask, target_path, resc_logits, gtl_probs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
