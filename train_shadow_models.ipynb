{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import optuna\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from src.train_models import create_shadow_models_parallel\n",
    "from src.utils import print_yaml, calculate_logits_and_inmask\n",
    "from src.models.resnet18_model import ResNet18\n",
    "from src.dataset_handler import processDataset, loadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d43da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- #\n",
    "#  Load target model and metadata  #\n",
    "# -------------------------------- #\n",
    "target_folder = \"resnet-cinic10-c51d329813\"\n",
    "target_path = os.path.join(\"target\", target_folder)\n",
    "\n",
    "# Target Pickle Metadata .json\n",
    "metadata_pkl_path = os.path.join(target_path, \"model_metadata.pkl\")\n",
    "with open(metadata_pkl_path, \"rb\") as f:\n",
    "    metadata_pkl = pickle.load(f)\n",
    "    \n",
    "# Target Metadata .json\n",
    "metadata_path = os.path.join(target_path, \"metadata.json\")\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "print(\"All metadata loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "#   Load Train yaml   #\n",
    "# ------------------- #\n",
    "config = None\n",
    "with open(\"./train.yaml\") as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "    \n",
    "print(\"\\n-------------- Train config --------------\")\n",
    "print_yaml(train_config)\n",
    "\n",
    "# Update the train config with target metadata (learning rate, batch size, etc)\n",
    "train_config['train']['epochs'] = metadata['train']['epochs']\n",
    "train_config['train']['batch_size'] = metadata['train']['batch_size']\n",
    "train_config['train']['learning_rate'] = metadata['train']['learning_rate']\n",
    "train_config['train']['momentum'] = metadata['train']['momentum']\n",
    "train_config['train']['t_max'] = metadata['train']['t_max']\n",
    "train_config['train']['drop_rate'] = metadata['train']['drop_rate']\n",
    "train_config['train']['model'] = metadata['train']['model']\n",
    "train_config['train']['optimizer'] = metadata['train']['optimizer']\n",
    "\n",
    "print(\"\\n-------------- Updated train config --------------\")\n",
    "print_yaml(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504448ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------#\n",
    "#  Prepare dataset  #\n",
    "#-------------------#\n",
    "data_cfg = train_config['data']\n",
    "\n",
    "dataset_name = data_cfg[\"dataset\"]\n",
    "root = data_cfg.get(\"root\", data_cfg.get(\"data_dir\"))\n",
    "dataset_path = os.path.join(root, dataset_name + \".pkl\")\n",
    "print(f\"dataset path: {dataset_path}\")\n",
    "\n",
    "trainset, testset, full_dataset = loadDataset(data_cfg)\n",
    "\n",
    "# Will split the dataset to use the same in indices as the baseline target model\n",
    "train_dataset, test_dataset, train_indices, test_indices = processDataset(data_cfg, trainset, testset, dataset=full_dataset)\n",
    "\n",
    "# Retrieve the targets\n",
    "full_dataset = train_dataset.dataset\n",
    "labels = full_dataset.targets\n",
    "print(f\"Length of dataset targets/labels: {len(labels)}\")\n",
    "print(f\"First 10 targets/labels: {labels[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf34207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "#   Train the Shadow models   #\n",
    "# --------------------------- #\n",
    "gpu_ids = [0, 1, 2, 3, 4, 5, 6]\n",
    "results = create_shadow_models_parallel(train_config, 256, gpu_ids, full_dataset, target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------- #\n",
    "#   STANDALONE Shadow Model, Metadata  and Dataset Loader used for calculating Logits and in_mask   #\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "# ---------------------------- #\n",
    "#   Load Dataset from Pickle   #\n",
    "# ---------------------------- #\n",
    "# HAVE TO GET DATASET FROM TARGET METADATA IF TO AUTOMATE\n",
    "dataset_name = \"cifar10\"\n",
    "print(f\"Dataset used by target model: {dataset_name}\")  \n",
    "\n",
    "reload_dataset = True\n",
    "if reload_dataset:\n",
    "    data_path = \"data\"\n",
    "    dataset_pkl_path = os.path.join(data_path, dataset_name + \".pkl\")    \n",
    "    with open(dataset_pkl_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # Wrap dataset if needed\n",
    "        \n",
    "    if not isinstance(dataset, CifarInputHandler.UserDataset):\n",
    "        data_tensor, target_tensor = dataset\n",
    "        dataset = CifarInputHandler.UserDataset(data_tensor, target_tensor)\n",
    "\n",
    "# ------------------------------------------------------- #\n",
    "#   Load Shadow Models and Calculate Logits and in_mask   #\n",
    "# ------------------------------------------------------- #\n",
    "# Path to the raw shadow models\n",
    "shadow_models_path = \"shadow_models/attack_objects/shadow_model\"\n",
    "\n",
    "# Path to processed shadow models\n",
    "sm_path = os.path.join(\"processed_shadow_models\", target_folder)\n",
    "os.makedirs(sm_path, exist_ok=True)\n",
    "\n",
    "reload_shadow_models = True\n",
    "if reload_shadow_models:\n",
    "    i = 0\n",
    "    while True:\n",
    "        model_pkl = os.path.join(shadow_models_path, f\"shadow_model_{i}.pkl\")\n",
    "        metadata_pkl = os.path.join(shadow_models_path, f\"metadata_{i}.pkl\")\n",
    "\n",
    "        if not os.path.exists(model_pkl) or not os.path.exists(metadata_pkl):\n",
    "            break  # stop when no more models\n",
    "\n",
    "        print(f\"Loading shadow model {i}\")\n",
    "\n",
    "        # Load model weights\n",
    "        state_dict = torch.load(model_pkl, map_location=\"cpu\")\n",
    "\n",
    "        # AVAILABLE PARAM CHECKS: model_class, online, init_params[\"num_classes\"\"]\n",
    "        # Reinstantiate model\n",
    "        num_classes = 10  # adapt if CIFAR-100\n",
    "        model = ResNet18(num_classes=num_classes)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        # Load metadata\n",
    "        with open(metadata_pkl, \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        calculate_logits_and_inmask(dataset, model, metadata, sm_path, idx=i)\n",
    "        \n",
    "        # Clean up\n",
    "        del metadata\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "print(\"\\nAll shadow model logits computed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
