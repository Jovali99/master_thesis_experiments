{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch import tensor, cat, save, load, optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "import src.study_handler as sh\n",
    "import src.save_load as sl\n",
    "from src.utils import print_yaml\n",
    "import multiprocessing\n",
    "from src.study_handler import run_baseline_optimization\n",
    "from src.dataset_handler import processDataset, loadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a81b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------#\n",
    "#  Open Config  #\n",
    "#---------------#\n",
    "config = None\n",
    "with open(\"./study.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "print_yaml(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------#\n",
    "#  Prepare dataset  #\n",
    "#-------------------#\n",
    "data_cfg = config['data']\n",
    "\n",
    "trainset, testset, full_dataset = loadDataset(data_cfg)\n",
    "if full_dataset is None:\n",
    "    train_dataset, test_dataset, train_indices, test_indices = processDataset(data_cfg, trainset, testset, None, None)\n",
    "else:\n",
    "    train_dataset, test_dataset, train_indices, test_indices = processDataset(data_cfg, None, None, None, full_dataset)\n",
    "\n",
    "# Assign the datasets\n",
    "sh.train_dataset = train_dataset\n",
    "sh.test_dataset = test_dataset\n",
    "print(f\"train_dataset data shape: {sh.train_dataset.dataset.data.shape}\")\n",
    "print(f\"train_dataset targets shape: {sh.train_dataset.dataset.targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------#\n",
    "#        Run study        #\n",
    "# ------------------------#\n",
    "gpu_ids = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "def parallell_optimization(config, gpu_ids):\n",
    "    study_cfg = config['study'] \n",
    "    print(f\"Starting parallell optimization using the following gpu ids: {gpu_ids}\")\n",
    "\n",
    "    metadata = sl.buildStudyMetadata(study_cfg, config['data']) \n",
    "    hash_id, save_path = sl.saveStudy(metadata, savePath=study_cfg['root'])\n",
    "    \n",
    "        \n",
    "    metadata = sl.buildStudyMetadata(study_cfg, config['data'])\n",
    "    hash_id, save_path = sl.saveStudy(metadata, savePath=config['study']['root'])\n",
    "    \n",
    "    # split up the trials among the gpus\n",
    "    total_trials = study_cfg[\"trials\"]\n",
    "    n_gpus = len(gpu_ids)\n",
    "    base_trials, remainder = divmod(total_trials, n_gpus)\n",
    "    \n",
    "    # assign trials per GPU\n",
    "    trials_per_gpu = [base_trials + 1 if i < remainder else base_trials for i in range(n_gpus)]\n",
    "\n",
    "    processes = []\n",
    "    for gpu_id, trials in zip(gpu_ids, trials_per_gpu):\n",
    "        print(f\"Running {trials} on gpu: {gpu_id}\")\n",
    "        p = multiprocessing.Process(\n",
    "            target=run_baseline_optimization,\n",
    "            args=(config, gpu_id, trials, save_path, hash_id)\n",
    "        )\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.start() \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        \n",
    "    db_path = os.path.join(study_cfg['root'], \"baseline_study.db\")\n",
    "    storage = f\"sqlite:///{db_path}\"\n",
    "    study_name = f\"{study_cfg['study_name']}-{hash_id}\"\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    return study\n",
    "\n",
    "if config is not None:\n",
    "    #run_optimization(config)\n",
    "    parallell_optimization(config, gpu_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
